在数千个处理器上运行的并行应用程序必须保护自己避免不可避免的系统故障。许多应用程序通过检查点将自己与失败隔离开来。对于许多应用程序，将检查点指向共享的单个文件是最方便的。使用这种方法，写操作的大小通常较小，并且与文件系统边界不一致。不幸的是，对于这些应用程序来说，这种首选的数据布局会导致底层文件系统性能非常差，而底层文件系统是为对非共享文件的大的、对齐的写操作而优化的。为了解决这种根本的不匹配，我们开发了虚拟并行日志结构文件系统PLFS。PLFS将应用程序的首选数据布局重新映射为针对底层文件系统进行优化的数据布局。通过在PanFS、Lustre和GPFS上的测试，我们已经看到，对于几个重要的基准测试和实际应用程序，这一层间接和重组可以将检查点时间减少一个数量级，而无需对应用程序进行任何修改。

常见的检查点模式。该图显示了三种基本的检查点模式:从左到右，N−N,N−1分段，N−1条带。在每个模式中，并行应用程序都是相同的，由分布在三个计算节点上的6个进程组成，每个计算节点都有3个状态块到检查点。这三种模式的不同之处在于应用程序状态在磁盘上的逻辑组织方式。在N−N模式中，每个进程将其状态保存到一个唯一的文件中。N−1分段是简单地将多个N−N文件连接成单个文件。最后，N−1 strided也使用单个文件，就像N−1分段，为每个块有一个区域，而不是每个进程有一个区域。从并行文件系统的角度来看，N−1 strided是最具挑战性的模式，因为它最可能导致小的、分散的和未对齐的写操作。注意，之前的工作[16]将N−N称为每个进程的文件，而N−1称为共享文件，但与我们的分段和分段术语相同。

我们首先假设，插入层可以透明地将一个N-1检查点模式重新安排为一个N-N模式，从而利用通过N-N模式可以增加的带宽来减少检查点时间。为了测试这个假设，我们开发了一个这样的插入层，PLFS，它是专门为大型的N-1并行检查点文件设计的。基本体系结构如图4所示。PLFS的原型是FUSE[1]，这是一个用于在非特权模式下运行可堆叠文件系统[49]的框架。

PLFS是一个虚拟的FUSE文件系统，挂载在计算节点上，位于并行应用程序和负责实际数据存储的底层并行文件系统之间。由于PLFS是一个虚拟文件系统，它利用了底层并行文件系统提供的许多服务，比如冗余、高可用性和全球分布的数据存储。这解放了PLFS，使其只专注于一项专门的任务:重新安排应用程序数据，使N-1写模式更适合底层并行文件系统。在本文的其余部分中，我们通常将PLFS指的是这个虚拟文件系统，它本身由一组跨计算系统运行的PLFS服务器组成，这些服务器由一个底层并行文件系统绑定在一起;当我们提到一个特定的PLFS时，我们指的只是这些服务器中的一个。

数据重组。此图描述了PLFS如何将一个N - 1条带检查点文件重新组织到底层并行文件系统上。由三个计算节点上的六个进程组成的并行应用程序由最上面的三个方框表示。每个框表示一个计算节点，一个圆表示一个进程，每个进程下面的三个小框表示该进程的状态。这些进程在PLFS上创建一个名为checkpoint1的新文件，从而导致PLFS在底层并行文件系统上创建一个容器结构。容器由一个也称为checkpoint1的顶级目录和几个用于存储应用程序数据的子目录组成。对于每个打开文件的进程，PLFS在其中一个子目录中创建一个数据文件，它还在同一个子目录中创建一个索引文件，该索引文件由计算节点上的所有进程共享。对于每次写操作，PLFS将数据追加到相应的数据文件中，并将一条记录追加到相应的索引文件中。该记录包含写入的长度、逻辑偏移量和一个指向它被追加到的数据文件中的物理偏移量的指针。为了满足读取，PLFS聚合这些索引文件，为逻辑文件创建一个查找表。图中还显示了用于存储有关逻辑文件的所有权和特权信息的access文件，以及openhosts和metadata子目录，它们缓存元数据以提高查询时间(例如stat调用)。

PLFS的基本操作如下。对于创建的每个逻辑PLFS文件，PLFS在底层并行文件系统上创建一个容器结构。在内部，容器的基本结构是一个层次目录树，由一个顶级目录和出现在用户面前的多个子目录组成;plf构建一个单一文件从这个容器结构的逻辑视图的方式类似于苹果的核心理念包[4]在Mac OS x多个进程打开相同的写作逻辑文件共享容器虽然容器中的每个打开获得独特的数据文件中所有的附加上写道。通过让并行应用程序中的每个写进程访问非共享数据文件，PLFS将N-1写访问模式转换为N-N写访问模式。当进程写入文件时，写入操作被附加到它的数据文件中，并且标识写入操作的记录被附加到一个索引文件中(如下所述)。

以这种方式重新排列数据可以提高写带宽，但也会增加读的复杂性。为了读取逻辑文件，PLFS为每个计算节点维护一个索引文件，记录每次写入的逻辑偏移量和长度。然后，PLFS可以通过将多个索引文件聚合到偏移量查找表中来构造全局索引。当缓存的元数据不可用时，根据需要构造这个全局偏移量来满足读取操作和查找操作(将在3.2节中讨论)。

构造全局索引的一个困难来自可能并发地写入相同偏移量的多个进程。这些进程无法知道哪一个将是最终写入器，因此它们需要同步以确定适当的顺序。这种同步需要公开给文件系统，才能成为有效的[13]。如果逻辑时钟[18]与这些同步相关联，则可以将其值写入索引文件，以便合并进程能够正确地确定写顺序。由于并行应用程序与由所有进程调用的barrier进行同步，因此简单地计数同步调用就足够了。在实践中，检查点没有重叠写操作，因此此时PLFS还没有实现重叠解决。

一个有趣的细微差别是，PLFS为每个进程提供一个数据文件，但每个计算节点上的所有进程只共享一个索引文件。共享索引文件很容易;当PLFS看到写操作时，它们已经被操作系统合并到一个内存空间中。操作系统将所有写操作发送到一个PLFS进程，该进程确保索引记录是正确的，并按时间顺序追加。由于当前的LANL应用程序在一个节点上运行多达16个进程，因此使用单个索引大大减少了容器中的文件数量;在下一个LANL超级计算机上，这可能达到64个。我们尝试以类似的方式减少数据文件的数量。写入带宽没有受到影响，但对于以写入文件的相同访问模式访问文件的统一重启，读取速度会变慢。
单个读取器按顺序访问单个数据文件的模式非常适合预取。但是，由于写入和读取阶段之间的时间差异，在一次统一重启中多个进程可能并不总是顺序地从一个共享文件中读取。

在描述了PLFS的基本操作之后，现在我们将更详细地介绍它的一些实现。虽然有许多可用的插入技术([43]包括一个调查)，我们选择熔断器有几个原因。因为保险丝允许plf通过一个标准的文件系统接口,应用程序可以使用它,而不用修改和文件上plf可以访问现有的整套工具,如ls、diff,和cp。除了提供用户透明,使用保险丝大大简化我们的开发工作。用户空间中的文件系统比内核文件系统更容易开发，而且更易于移植。然而，这种便利并不是免费的，因为FUSE确实增加了一些开销，如第4节所示

因为PLFS尽可能地利用底层并行文件系统，所以我们为容器提供与PLFS文件相同的逻辑名称。这允许PLFS将readdir系统调用直接传递给底层并行文件系统，并返回结果而不需要任何转换。PLFS还可以在不进行任何转换的情况下处理mkdir系统调用。以这种方式利用底层并行文件系统要求PLFS使用其他一些机制来区分常规目录和容器，以便正确地实现stat系统调用。由于SUID位很少在目录中使用，但是底层文件系统允许设置，PLFS在容器上设置这个位;然而，这意味着PLFS必须不允许在常规目录上设置此位。

正如我们前面讨论的，并行应用程序执行同步检查点;对PLFS来说，这意味着在多个计算节点上运行的多个进程写一个N-1检查点文件将导致每个计算节点上的PLFS尝试在底层并行文件系统上并发地创建相同的容器。出现这种困难的原因是，每个PLFS必须首先统计该路径，以查看该路径是否可用，该容器是否已经存在，或者该位置是否有一个常规目录。理想情况下，每个PLFS可以统计该路径，当该位置为空时，自动创建一个设置了SUID位的目录。因此，每个PLFS必须首先创建一个目录，然后设置SUID位。这样做会导致竞态条件:如果一个PLFS在另一个PLFS创建目录之后统计路径，但是在它设置SUID位之前，那么该统计将表明该位置中有一个常规目录。发出打开逻辑文件的应用程序随后将收到一个错误，该错误指示在该位置已经有一个目录。为了避免这种竞争条件，每个PLFS首先创建一个具有唯一名称的隐藏目录，设置其SUID位，然后原子地将其重命名为原始容器名。

对文件的元数据操作包括访问它的权限(包括SUID)、它的容量、它最后一个字节的偏移量和它最后一次更新的时间戳。对于PLFS上的目录，它们是由底层文件系统提供的。但是对于像图4这样由容器构造的PLFS文件，这些元数据操作必须从容器内的许多底层文件中计算。

因为容器本身上的SUID位已被重载，以指示该目录根本不是目录，而是一个容器，因此它也不能用于指示用户是否在容器所表示的PLFS文件上设置了SUID。相反，我们使用容器内的一个文件，即访问文件，来表示适当的SUID和与容器关联的其他权限。例如，chmod和chgrp被定向到逻辑文件，它们被应用到容器内的访问文件。

逻辑文件的容量是容器内文件容量的总和。最后一个更新时间戳是最后一个更新时间戳的最大值。最后一个字节的偏移量是在任何索引文件中记录的最大逻辑偏移量。

通过对PLFS文件的每次stat调用计算这些总和和最大值是非常昂贵的。我们的加速策略是将最近计算的值缓存到metadata子目录中。为了使这个缓存尽可能有效，我们让每个PLFS服务器在该节点上的最后一个写入器关闭文件时将其内存数据结构中的所有信息缓存到这个元数据子目录中。在此关闭过程中，FUSE进程创建一个名为H.L.B.的文件T，其中H -是节点的主机名，L是该节点上记录的最后一个偏移量，B是该节点管理的所有文件容量的总和，T是这些文件中的最大时间戳。

当没有进程打开此容器进行写操作时，对容器的stat调用会在元数据子目录上发出一个readdir，然后报告最后一个字节偏移量的最大偏移量、修改时间戳的最大时间戳以及容量块的总和。

如果一个或多个进程打开容器以便写入，则相应的缓存元数据值可能过时。因此，当节点上的一个或多个进程打开该文件以便写入时，PLFS客户机在openhosts子目录中创建一个文件，并在所有打开的文件都关闭后删除该文件。stat还必须在openhosts上执行readdir，以发现是否有节点打开了要写入的文件，从而确定哪些元数据缓存项可能已经过时。

当openhosts子目录中有主机名文件时，执行stat调用的节点可以读取与这些主机名关联的索引文件的内容，以找到最大的逻辑偏移量，然后将其与未过期的元数据缓存文件结合起来。

根据HPC社区的经验，统计打开的文件几乎总是由试图监视其工作进度的用户来完成的。他们想要的是一个廉价的显示进程的探针，而不是一个昂贵的即时正确的值[37]。按照这个逻辑，PLFS不会读取和处理与打开容器供写的主机名相关联的索引文件。相反，它假设文件不是稀疏的(即每个字节都被写入)，并将容器内所有数据文件的大小相加，以估计PLFS文件的最后偏移量。因为对每个数据的写入总是简单地追加，这个估计将单调地增加额外的数据写入到文件中，允许用户监视进程。当容器关闭时，元数据子目录包含完全正确的缓存值，并且在容器没有进程写入它时始终提供完全的准确性。

